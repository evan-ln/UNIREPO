---
title: "Music Data Dashboard"
output: 
  flexdashboard::flex_dashboard:
    theme: "cosmo"
date: "2025-14-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plotly)
library(readr)
library(dplyr)
library(tuneR)
library(seewave)
library(av)
library(tidyverse)


# Install required packages (if not already installed)
if (!requireNamespace("tuneR", quietly = TRUE)) install.packages("tuneR")
if (!requireNamespace("seewave", quietly = TRUE)) install.packages("seewave")
if (!requireNamespace("av", quietly = TRUE)) install.packages("av")

# Convert MP3 to WAV
av_audio_convert("evan-l-1.mp3", "evan-l-1.wav", format = "wav")
av_audio_convert("evan-l-2.mp3", "evan-l-2.wav", format = "wav")


# Set feature directory
features_dir <- 'features'

# Source
source('compmus.R')

# Load the dataset
compmus_data <- read_csv("compmus2025.csv")

# My tracks
my_tracks <- c("evan-l-1", "evan-l-2")
json_my_tracks <- c("features/evan-l-1.json", "features/evan-l-2.json")

```

Dash Version 3.0
---

ISSUES:
- can't download the low-level features


# Tempo Analysis


## Histogram for tempo class corpus
```{r tempo_analysis, echo=FALSE}
# Histogram of tempo for the class corpus
plotly::ggplotly(
compmus_data %>%
  ggplot(aes(x = tempo)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Tempo - Class", x = "Tempo (BPM)", y = "Count")
)
```
From the histogram, we can see that the most frequent tempo (mode) in the class corpus is 85 BPM, with a count of 16.

85 BPM is relatively slow and could correspond to genres like hip-hop, downtempo electronic, or ballads.It also corresponds with the human heartbeat, which could explain a natural preference. The data displays several smaller peaks, this could hint at clusters of similar music.


---


## Chordograms for own songs

```{r tempograms, echo=FALSE}
# Generate tempograms for user contributions
for (track in json_my_tracks) {
  tempogram <- tryCatch({
    compmus_tempogram(track, window_size = 8, hop_size = 1, cyclic = FALSE)
  }, error = function(e) {
    message(paste("Error processing file:", track))
    return(NULL)
  })
  
  if (!is.null(tempogram)) {
    print(
      ggplot(tempogram, aes(x = time, y = bpm, fill = power)) +
        geom_raster() +
        scale_fill_viridis_c(guide = "none") +
        labs(title = paste("Tempogram of", track), x = "Time (s)", y = "Tempo (BPM)") +
        theme_classic()
    )
  }
}
```
Track 1: The first track, which is the ambient track, appears pretty uniform, smooth and continuous until the second minute, where it gets progressively broken up. This could be due to some subtle changes in the sound layers, while the song lacks a strong beat (as is often the case for ambient music).

Track 2: The second track, which is the breakbeat track, displays multiple evenly spaced lines. There is little variation visible aside from a small bridge, as the track mostly has a repetitive beat.




---


# Chordogram - 

```{r chordograms, echo=FALSE}

# Load the chroma.r file where the keys and other components are defined

# Function to create the keygram plot (keys on the Y-axis)
create_keygram <- function(file) {
  # Process the audio file with chroma and match pitch templates for key estimation
  chroma <- tryCatch({
    compmus_chroma(file, norm = "identity")
  }, error = function(e) {
    print(paste("Error processing file:", file))
    return(NULL)
  })
  
  # If chroma extraction fails, return
  if (is.null(chroma)) return(NULL)
  
  # Match chroma data to key templates
  keygram <- compmus_match_pitch_templates(
    chroma,            # Chroma data
    key_templates,     # Key templates (already loaded from chroma.r)
    norm = "identity", # Normalization setting
    distance = "cosine" # Distance metric for matching
  )
  
  # Plot the keygram with keys on the Y-axis
  ggplot(keygram, aes(x = time, y = name, fill = d)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = "none") +  # Color scale
    labs(x = "Time (s)", y = "Key", fill = NULL) +
    theme_classic() +  # Clean theme
    ggtitle(paste("Keygram of", file)) +    # Title for the plot
    theme(plot.title = element_text(size = 12))  # Adjust title size
}

#doesn't work???
par(mfrow = c(2, 1))  # Display side-by-side
create_keygram("evan-l-1.wav")  # Keygram for the first track
create_keygram("evan-l-2.wav")  # Keygram for the second track


```


---


# Chromagram

## Chromagrams for own tracks

**!!! I currently keep having issues with my chromagrams not loading (though they do show in R), so it may be that they don't show up yet. Trying to fix this ASAP.**


Chromagrams capture the harmonic content by showing how energy is distributed across the 12 pitch classes over time.

```{r chromagrams, echo=FALSE}
#currently has issues loading.. why?

compmus_chroma()

#change up the compmus file + add from tutorial

plot_chromagram <- function(file) {
  wave <- readWave(file)
  dev.new()  # To open a new graphic
  spectro(wave, wl = 1024, ovlp = 75, collevels = seq(-50, 0, 5), main = paste("Chromagram -", file))
}


par(mfrow = c(2,1))
plot_chromagram("evan-l-1.wav")
plot_chromagram("evan-l-2.wav")
```
- **Evan-l-1.wav - Ambient **
  - The energy is mainly in the lower frequency range, below ~5 kHz
  - The overall amplitude is low, this reflects the quiet ambient sounds
  - The minimal structure suggests an ambient or instrumental track

- **Evan-l-2.wav - Breakbeat **
  - Covers a much wider frequency range, reaching beyond 10 kHz, with distinct variations in energy
  - Brighter colors indicate stronger amplitude, due to the track being comparatively dynamic
  - The structure implies prominent rhythmic and melodic elements, as is the case with my breakbeat track

---


```{r self_similarity, echo=FALSE}
compute_ssm <- function(file, feature = "chroma") {
  wave <- readWave(file)
  
  # Extract the signal
  signal <- wave@left 
  sr <- wave@samp.rate  # Get sample rate
  
  if (feature == "chroma") {
    chroma <- melfcc(wave, numcep = 12)  # No `$cep`
    dist_matrix <- as.matrix(dist(t(chroma)))  # Convert to self-similarity matrix
  } else if (feature == "timbre") {
    timbre <- melfcc(wave, numcep = 20)
    dist_matrix <- as.matrix(dist(t(timbre)))
  }
  
  return(dist_matrix)
}

ssm_chroma_1 <- compute_ssm("evan-l-1.wav", feature = "chroma")
ssm_timbre_1 <- compute_ssm("evan-l-1.wav", feature = "timbre")
ssm_chroma_2 <- compute_ssm("evan-l-2.wav", feature = "chroma")
ssm_timbre_2 <- compute_ssm("evan-l-2.wav", feature = "timbre")

par(mfrow = c(2,2))
image(ssm_chroma_1, main = "Chroma SSM - Evan-l-1.wav", col = gray.colors(100))
image(ssm_timbre_1, main = "Timbre SSM - Evan-l-1.wav", col = gray.colors(100))
image(ssm_chroma_2, main = "Chroma SSM - Evan-l-2.wav", col = gray.colors(100))
image(ssm_timbre_2, main = "Timbre SSM - Evan-l-2.wav", col = gray.colors(100))
```

## SSMs


- **Evan-l-1.wav - Ambient**
  - The chroma-SSM shows some variation, but overall pretty uniform and repetitive
  - The timbre-SSM is mostly smooth and table

- **Evan-l-2.wav - Breakbeat**
  - The chroma-SSM has more distinct patterns, like a checkerboard. The song has stronger harmonic changes with varied chord progressions.
  - The timbre-SSM shows greater shifts in instruments or timbres


---

# AI-Generated Tracks

These tracks were created using **Stableaudio AI** ([Stableaudio](https://stableaudio.com/generate)). I took inspiration from genre tags on **RateYourMusic** and carefully crafted prompts using detailed descriptors to shape the sound. After generating the tracks, I simply downloaded the MP3 files.

**Track 1: Meditative Ambient Soundscape**

**Style:** Ambient, Post-Rock, Cinematic  
**Length:** 2 minutes  
**Goal:** A calm, meditative ambient with minimal instrumentation.  

**Tags Used:**  
Ambient, Post-Rock, Cinematic, Ethereal, Soothing, Meditative, Minimalist, Warm Subtle Bass, Deep Drones, Airy Pads, Textures, Analog Synths, Field Recordings, Wind Sounds, Reverb, **60 BPM**
---

**Track 2: Energetic Breakbeat Rave**

**Style:** Breakbeat, Acid Breaks, 90s Rave  
**Length:** 2 minutes  
**Goal:** A high-energy, chaotic breakbeat track.  

**Tags Used:**  
Breakbeat, Acid Breaks, 90s Rave, Energetic, Raw, Funky, Chaotic, Breakbeats, Deep Bass, Distorted 808, Acid Bass, Filtered Chords, Reversed Pads, Vocal Chops, **135 BPM**

---

**Track Creation**

1. **Prompt Design:**  
   - Used RateYourMusic to explore fitting **genre tags**.  
   - Structured prompts with **specific instruments, moods, and BPM** to guide the AI.  
2. **Generation:**  
   - Entered prompts into **Stableaudio AI**.  
   - Experimented with variations before selecting the best versions.  
3. **Finalization:**  
   - Downloaded the **MP3 files** and ensured they matched my vision.  =

---

# Visualization

Here's a scatterplot of the Danceability compared to the Tempo of the tracks. My track 1 (ambient) is marked red, track 2 (breakbeat) is blue.

```{r danceability_plot, echo=FALSE, fig.width=6, fig.height=4}
dance_plot <- ggplot(compmus_data, aes(x = danceability, y = tempo, color = danceability, text = filename)) +
  geom_point(alpha = 0.6, size = 2) +  
  geom_point(data = compmus_data %>% filter(filename == "evan-l-1"), 
             aes(x = danceability, y = tempo, text = filename), color = "red", size = 2.5) +
  geom_point(data = compmus_data %>% filter(filename == "evan-l-2"), 
             aes(x = danceability, y = tempo, text = filename), color = "blue", size = 2.5) +
  
  labs(title = "Danceability vs. Tempo",
       x = "Danceability",
       y = "Tempo",
       color = "Danceability") +
  
  theme_classic()

# Adjust plot size + hover tooltips show song titles
ggplotly(dance_plot, tooltip = "text")
```

---

# Final Thoughts

There appears to be no set correlation between the danceability and tempo of the tracks. However, an interesting pattern emerges: there are two clusters—one with low danceability, and another with high danceability, while the tempo does not differ much.

Regarding my own tracks:

- The **calm ambient track** has an average tempo and low danceability.
- The **breakbeat song** has an average tempo but high danceability.

One particularly surprising observation is how the AI interpreted the second song’s tempo. While I set it to **135 BPM**, it was classified as **93 BPM**. This suggests that the AI might have emphasized a different rhythmic structure or half-time feel in its classification.
