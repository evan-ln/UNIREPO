---
title: "Music Data Dashboard"
output: 
  flexdashboard::flex_dashboard:
    theme: "cosmo"
date: "2025-02-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plotly)
library(readr)
library(dplyr)
library(tuneR)
library(seewave)
library(av)

# Install required packages (if not already installed)
if (!requireNamespace("tuneR", quietly = TRUE)) install.packages("tuneR")
if (!requireNamespace("seewave", quietly = TRUE)) install.packages("seewave")
if (!requireNamespace("av", quietly = TRUE)) install.packages("av")

# Convert MP3 to WAV
av_audio_convert("evan-l-1.mp3", "evan-l-1.wav", format = "wav")
av_audio_convert("evan-l-2.mp3", "evan-l-2.wav", format = "wav")

# Load the dataset
compmus_data <- read_csv("compmus2025.csv")

# Define my tracks
my_tracks <- c("evan-l-1", "evan-l-2")
```



# Overview

This dashboard explores the musical properties of my AI-generated tracks using chromagrams, self-similarity matrices, and danceability analysis.



---

## Chromagram

### Chromagrams for own tracks

Chromagrams capture the harmonic content by showing how energy is distributed across the 12 pitch classes over time.

```{r chromagrams, echo=FALSE}
plot_chromagram <- function(file) {
  wave <- readWave(file)
  spectro(wave, wl = 1024, ovlp = 75, collevels = seq(-50, 0, 5), main = paste("Chromagram -", file))
}

par(mfrow = c(2,1))
plot_chromagram("evan-l-1.wav")
plot_chromagram("evan-l-2.wav")
```

---

## Self-Similarity Matrices

```{r self_similarity, echo=FALSE}
compute_ssm <- function(file, feature = "chroma") {
  wave <- readWave(file)
  
  # Extract the signal
  signal <- wave@left  # Use left channel if stereo
  sr <- wave@samp.rate  # Get sample rate
  
  if (feature == "chroma") {
    chroma <- melfcc(wave, numcep = 12)  # No `$cep`
    dist_matrix <- as.matrix(dist(t(chroma)))  # Convert to self-similarity matrix
  } else if (feature == "timbre") {
    timbre <- melfcc(wave, numcep = 20)
    dist_matrix <- as.matrix(dist(t(timbre)))
  }
  
  return(dist_matrix)
}

ssm_chroma_1 <- compute_ssm("evan-l-1.wav", feature = "chroma")
ssm_timbre_1 <- compute_ssm("evan-l-1.wav", feature = "timbre")
ssm_chroma_2 <- compute_ssm("evan-l-2.wav", feature = "chroma")
ssm_timbre_2 <- compute_ssm("evan-l-2.wav", feature = "timbre")

par(mfrow = c(2,2))
image(ssm_chroma_1, main = "Chroma SSM - Evan-l-1.wav", col = gray.colors(100))
image(ssm_timbre_1, main = "Timbre SSM - Evan-l-1.wav", col = gray.colors(100))
image(ssm_chroma_2, main = "Chroma SSM - Evan-l-2.wav", col = gray.colors(100))
image(ssm_timbre_2, main = "Timbre SSM - Evan-l-2.wav", col = gray.colors(100))
```

---

## AI-Generated Tracks

These tracks were created using **Stableaudio AI** ([Stableaudio](https://stableaudio.com/generate)). I took inspiration from genre tags on **RateYourMusic** and carefully crafted prompts using detailed descriptors to shape the sound. After generating the tracks, I simply downloaded the MP3 files.

### **Track 1: Meditative Ambient Soundscape**

**Style:** Ambient, Post-Rock, Cinematic  
**Length:** 2 minutes  
**Goal:** A calm, meditative ambient with minimal instrumentation.  

### **Track 2: Energetic Breakbeat Rave**

**Style:** Breakbeat, Acid Breaks, 90s Rave  
**Length:** 2 minutes  
**Goal:** A high-energy, chaotic breakbeat track.  

---

## Visualization

Here's a scatterplot of the Danceability compared to the Tempo of the tracks. My track 1 (ambient) is marked red, track 2 (breakbeat) is blue.

```{r danceability_plot, echo=FALSE, fig.width=6, fig.height=4}
dance_plot <- ggplot(compmus_data, aes(x = danceability, y = tempo, color = danceability, text = filename)) +
  geom_point(alpha = 0.6, size = 2) +  
  geom_point(data = compmus_data %>% filter(filename == "evan-l-1"), 
             aes(x = danceability, y = tempo, text = filename), color = "red", size = 2.5) +
  geom_point(data = compmus_data %>% filter(filename == "evan-l-2"), 
             aes(x = danceability, y = tempo, text = filename), color = "blue", size = 2.5) +
  labs(title = "Danceability vs. Tempo",
       x = "Danceability",
       y = "Tempo",
       color = "Danceability") +
  theme_classic()

# Adjust plot size + hover tooltips show song titles
ggplotly(dance_plot, tooltip = "text")
```

---

## Final Thoughts

There appears to be no set correlation between the danceability and tempo of the tracks. However, an interesting pattern emerges: there are two clusters—one with low danceability, and another with high danceability, while the tempo does not differ much.

Regarding my own tracks:

- The **calm ambient track** has an average tempo and low danceability.
- The **breakbeat song** has an average tempo but high danceability.

One particularly surprising observation is how the AI interpreted the second song’s tempo. While I set it to **135 BPM**, it was classified as **93 BPM**. This suggests that the AI might have emphasized a different rhythmic structure or half-time feel in its classification.
